项目流程
1.获取用户输入
2.根据用户输入调用本地接口，获取到查询到的相似分块数据
3.用分块数据，加入提示词中，调用大模型，获取到回答
4.将大模型返回的json原样返回

项目介绍：本项目为中继服务，旨在chatbox调用ollama的中间添加知识库流程，请求和返回结构都为ollama格式
接下来的序号与项目流程一致，主要描述具体功能和需求

1.建立服务，通过开放的接口获取用户输入，请求格式为
    {
        "model": "<model-name>",  // 模型名称
        "messages": [             // 消息列表
            {
            "role": "user",       // 用户角色
            "content": "<input-text>" // 用户输入
            }
        ],
        "stream": false,          // 是否启用流式响应
        "options": {              // 可选参数
            "temperature": 0.7,
            "max_tokens": 123456
        }
    }
2.本地接口
    http://127.0.0.1/v1/chunk/retrieval_test
    请求方式：POST
    请求参数：
    {
    "similarity_threshold": 0.2,
    "vector_similarity_weight": 0.8,
    "use_kg": false,
    "question": "2024年10月全国城市空气质量报告", //question此处为用户输入
    "doc_ids": [],
    "kb_id": "fb0f617ae96111efa7160242ac120006",//kb_id为知识库id，此处需要放到.env配置文件中配置
    "page": 1,
    "size": 10
    }
    返回格式：
    {
    "code": 0,
    "data": {
        "chunks": [
            {
                "chunk_id": "073ca5c0f8889c7b",
                "content_ltks": "2 titl 2024 11 222024 年 10 月 全国 城市 空气质量 报告 href httpwww mee gov cn hjzl dqhj cskqzlzkyb 202411 t 20241122 _ 1096004 shtml 3 titl 2024 10 222024 年 9 月 全国 城市 空气质量 报告 href httpwww mee gov cn hjzl dqhj cskqzlzkyb 202410 t 20241022 _ 1089906 shtml",
                "content_with_weight": "{\"2\": {\"title\": \"2024-11-222024年10月全国城市空气质量报告\", \"href\": \"https://www.mee.gov.cn/hjzl/dqhj/cskqzlzkyb/202411/t20241122_1096004.shtml\"}}{\"3\": {\"title\": \"2024-10-222024年9月全国城市空气质量报告\", \"href\": \"https://www.mee.gov.cn/hjzl/dqhj/cskqzlzkyb/202410/t20241022_1089906.shtml\"}}",
                "doc_id": "e1a86f86ee1311efbb640242ac120006",
                "docnm_kwd": "城市空气质量状况20250218.json",
                "image_id": "",
                "important_kwd": [],
                "kb_id": "fb0f617ae96111efa7160242ac120006",
                "positions": [],
                "similarity": 0.7051590910585064,
                "term_similarity": 0.6837876654024799,
                "vector_similarity": 0.7105019474725129
            }
        ],
        "doc_aggs": [
            {
                "count": 9,
                "doc_id": "e1a86f86ee1311efbb640242ac120006",
                "doc_name": "城市空气质量状况20250218.json"
            },
            {
                "count": 1,
                "doc_id": "e1b2a1b8ee1311efbb640242ac120006",
                "doc_name": "全国空气质量状况20250218.json"
            }
        ],
        "labels": null,
        "total": 235
    },
    "message": "success"
    }
    具体返回格式你自己解析一下吧
    content_with_weight需要放到提示词中 同时chunks不止会有一个数据会有多个 多个都需要放到提示词中
3.提示词为
    你是一个智能助手，请总结知识库的内容来回答问题，请列举知识库中的数据详细回答。当所有知识库内容都与问题无关时，你的回答必须包括“知识库中未找到您要的答案！”这句话。回答需要考虑聊天历史。
        以下是知识库：
        {knowledge} //这个地方替换为content_with_weight
        以上是知识库。
  调用模型为本地的ollama模型，模型地址 和模型名称需要放在配置文件中，请求格式为
    {
    "model": "<model-name>",  // 模型名称
    "messages": [             // 消息列表
        {
        "role": "user",       // 用户角色
        "content": "<input-text>" // 用户输入
        }
    ],
    "stream": false,          // 是否启用流式响应
    "options": {              // 可选参数
        "temperature": 0.7,
        "max_tokens": 100
    }
    }
    响应格式为
    {
        "message": {
            "role": "assistant",    // 助手角色
            "content": "<generated-text>" // 生成的文本
        },
        "done": true
    }
4.将大模型返回的json原样返回

